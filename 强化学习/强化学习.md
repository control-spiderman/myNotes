强化学习

定义：通过从交互中学习来实现目标的计算方法



强化学习系统要素

- 历史

- 状态
- 策略
  - 从状态到行动的映射
  - 确定性策略
  - 随机策略（stochastic policy）
- 奖励
  - 一个定义强化学习目标的标量
- 价值函数
  - 是对于未来累计奖励的预测
- 环境
  - 环境的模型（model）用于模拟
  - 环境的行为包括：
    - 预测下一个状态
    - 预测下一个奖励



强化学习智能体分类

- 基于模型的强化学习

- 模型无关的强化学习

强化学习智能体分类：

- 基于价值
- 基于策略
- actor-critic



探索与利用问题：以多臂老虎机为例

探索是尝试更多可能的动作，不管这个动作是否会得到最优奖励，这样可以更清楚所有动作的奖励情况。

利用是从已知的动作中，选择平均奖励最高的那个动作。由于已知信息只来自于有限次的观测，所以不一定会是最优值。

有一些经典的算法可以用来权衡探索和利用。分别是ε-贪心算法、UCB算法、汤普森采样算法

ε-贪心算法

UCB算法

汤普森采样算法

多臂老虎机问题



## 马尔可夫决策过程

与多臂老虎机不同，马尔可夫决策过程还包含了**状态信息以及状态之间的转移机制**

### 马尔可夫过程

#### 马尔可夫性质

马尔可夫性质是指某时刻的状态只取决于上一时刻的状态，用公式表示为：

$P(S_{t+1}|S_t) = P(S_{t+1}|S_1,...,S_t)$

#### 马尔可夫过程

马尔可夫过程用一个双元组表示<S,P>，S是有限数量的状态集合，P是状态转移矩阵。

加入有n个状态，即$\mathcal{S}=\{s_1,s_2,...,s_n\}$ ，状态转移矩阵为
$$
\mathcal{P} = \left[ \begin{matrix} p(s_1|s_1) & ... & p(s_n|s_1) \\... \\ p(s_1|s_n)  & ... & p(s_n|s_n) \end{matrix} \right]
$$
矩阵中的第i行第j列元素$p(s_j|s_i) = P(S_{t+1}=s_j|S_t=s_i)$ 表示从状态$s_i$转移到状态$s_j$ 的概率。从某个状态出发，到其他状态的概率和为1，即状态转移矩阵P的每一行和为1。

### 马尔可夫奖励过程

马尔可夫奖励过程是一个四元组$<\mathcal{S},\mathcal{P},\mathcal{r},\gamma>$ ，相比于马尔可夫过程，多了奖励函数r和折扣系数γ。

- 奖励函数r：某个状态s的奖励函数r(s)**表示转移到该状态s时可以获得的奖励**。
- 折扣系数γ：取值范围为[0,1)。在计算奖励的时候，当前时刻以后的利益(远期利益)具有不确定性，我们更希望能尽可能获取到奖励（即当前时刻的），所以需要对距离越久的奖励打更多的折扣。靠近1更关注长期累计奖励，靠近0更考虑短期奖励。

#### 回报

一个马尔可夫奖励过程中，从**某一状态$s_t$开始直到终止状态时所有奖励的衰减之和称为回报**：
$$
G_t = R_{t} + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum^{T}_{k=0} \gamma^k R_{t+k}
$$
例如选取$s_1$为起始状态，设置$\gamma$=0.5，**采样**到一条状态序列为$s_1-s_2-s_3-s_6$，就可以计算$s_1$的回报$G_1$，得到$G_1=-1+0.5\times (-2)+0.5^2\times (-2)=-2.5$

<img src="https://staticcdn.boyuai.com/user-assets/358/GqdgvfkCaBez4FEfFCYeJz/qgcd3cjbzb.png!png" width = "360" height = "360" align=center />

#### 价值函数

马尔可夫奖励过程中，**一个状态的期望回报即为这个状态的价值**，**所有状态的价值就组成了价值函数**。价值函数输入为某个状态，输出为这个状态的价值。价值函数写成$V(s)=\mathbb{E}[G_t|S_t=s]$，展开为：
$$
\begin{align}
V(s)&= \mathbb{E}[G_t|S_t=s]     \nonumber\\
  &= \mathbb{E}[R_{t}+\gamma R_{t+1}+\gamma^2R_{t+2}+...|S_t=s]  \nonumber \\
	&= \mathbb{E}[R_{t}+\gamma (R_{t+1}+\gamma R_{t+2}+...)|S_t=s]  \nonumber  \\
	&= \mathbb{E}[R_{t}+\gamma G_{t+1}|S_t=s]    \nonumber \\
	&=\mathbb{E}[R_{t}+\gamma V(S_{t+1})|S_t=s]  \nonumber	\\
	&=r(s) + \gamma \sum_{s' \in S}p(s'|s)V(s')
\end{align}
$$
最后一个等式即为**贝尔曼方程（Bellman Equation）**。

如果一个马尔可夫奖励过程有n个状态，则价值函数V可以表示为n个元素的列向量，同理奖励函数R也表示为一个列向量，则贝尔曼方程可以表示为矩阵的形式：
$$
\begin{align}
\mathcal{V} &=\mathcal{R}+\gamma \mathcal{PV}  \nonumber\\
\mathcal{V} &=(I-\gamma \mathcal{P})^{-1} \mathcal{R} \nonumber
\end{align}
$$
可以通过上式这个解析解计算价值函数。计算复杂度是$O(n^3)$，直接求解只适合较小的MRP，求解大规模MRP可以使用动态规划、蒙特卡洛方法、时序差分法等。

### 马尔可夫决策过程

马尔可夫决策过程是一个五元组$\langle\mathcal{S},\mathcal{A},p,\mathcal{r},\gamma\rangle$ ，在马尔可夫奖励过程中加入了动作A，此时奖励函数和状态转移函数都多了动作这个自变量。

- $\mathcal{r}(s,a)$是奖励函数，此时奖励同时取决于状态$s$和动作$a$
- $p(s'|s,a)$是状态转移函数，表示在状态$s$执行动作$a$之后到达状态$s'$的概率

不同于MRP，MDP中一般存在一个智能体(agent)来执行动作：

> MDP和MRP的区别（举个例子）：
>
> 如果一艘小船在大海中随着水流自由飘荡的过程就是一个马尔可夫奖励过程，它如果凭借运气漂到了一个目的地就能获得比较大的奖励；
>
> 如果有个水手在控制着这条船往哪个方向前进，就可以主动选择前往目的地获得比较大的奖励。因为这是一个与时间相关的不断进行的过程，在智能体和环境MDP之间存在一个不断交互的过程。

<img src="https://staticcdn.boyuai.com/user-assets/358/oQ8snB74AaoJvJNPawBqi7/qgfl65ncrh.png!png" width = "360" height = "360" align=center />

上图过程：**智能体根据当前状态$S_t$选择动作$A_t$；对于状态$S_t$和动作$A_t$，MDP根据奖励函数和状态转移函数得到$S_{t+1}$和$R_t$并反馈给智能体。智能体的目标是最大化得到的累计奖励。智能体根据当前状态从动作的集合$\mathcal{A}$中选择一个动作的函数，被称为策略。**

#### 策略

agent策略用$\pi$表示，策略$\pi(a|s)= p(A_t = a|S_t = s)$是一个函数。策略有确定性策略和随机性策略。

- 确定性策略：每个状态只输出一个确定性的动作

- 随机性策略：每个状态输出的是动作的分布，根据该分布采样一个动作

#### 状态价值函数

状态价值函数$V^{\pi}(s)$定义为**从状态s出发遵循策略$\pi$能获得的期望回报**：
$$
V^{\pi}(s) = \mathbb{E}_\pi[G_t|S_t=s]
$$

#### 动作价值函数

动作价值函数$Q^{\pi}(s,a)$定义为，**对当前状态s执行动作a得到的期望回报**：
$$
Q^{\pi}(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]
$$

#### 状态价值函数与动作价值函数关系

在给定策略$\pi$下，状态s的价值等价于使用该策略执行相应的动作所得到的期望回报与选择该策略的概率的乘积再求和：
$$
V^{\pi}(s) = \sum_{a \in A} \pi(a|s)Q^{\pi}(s,a)
$$
使用策略$\pi$ 时，对当前状态s执行动作a的价值等价于及时奖励加上衰减的下一可能状态的价值与转移到该状态的概率的乘积再求和：
$$
Q^{\pi}(s,a)=r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')
$$

#### 贝尔曼期望方程

在MRP中提及的贝尔曼方程基础上考虑上动作，对动作价值函数和状态价值函数稍作变形：
$$
\begin{align}
V^{\pi}(s) &= \mathbb{E}_\pi[R_{t}+\gamma V^{\pi}(S_{t+1})|S_t=s]  \nonumber    \\
&= \sum_{a \in A} \pi(a|s)\left( r(s,a) + \gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')\right) \nonumber
\end{align}
$$

$$
\begin{align}
Q^{\pi}(s,a)&=\mathbb{E}_\pi[R_{t}+\gamma Q^{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]   \nonumber \\
&= r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)\sum_{a' \in A} \pi(a'|s')Q^{\pi}(s',a')  \nonumber
\end{align}
$$

## 动态规划算法