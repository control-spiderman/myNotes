为什么不用离散式方法（one-hot）来进行词的向量表示？
TF-IDF是什么？可以解决什么问题？有什么优缺点？
Word2vec（glove）与elmo有什么区别？
语言模型（LM）是什么？
RNN为什么适用于文本处理？它存在什么问题？有什么解决方案？
LSTM有哪些门？他们有什么作用？
CNN和RNN在文本处理中有什么不同？
Seq2Seq是什么？常用于解决什么任务？
Relu有什么优缺点？

BERT有哪些子任务？
BERT的双向体现在哪里？
BERT模型中的激活函数是什么？有什么特点？
BERT是如何解决梯度消失的？

Transformer用到的LN是什么？

Self-attention是怎么计算的？

什么是BN？简述实现细节

LN和BN有什么不同？

为什么Transformer 需要进行 Multi-head Attention？

Transformer中在计算注意力时为什么要除以维度？

什么是欠拟合？如何解决？
什么是过拟合？如何解决？

什么是梯度消失和梯度爆炸？如何解决梯度消失和梯度爆炸？

什么是dropout？他解决了什么问题？

L1正则和L2正则是什么？它们解决了什么问题？

L2为什么可以解决过拟合？在使用的过程中需要考虑bias吗？
什么是large bias 和 large variance？会导致什么问题？如何解决？
什么是warmup？它解决了什么问题？
Python中的协程是什么？如何实现？

特征离散化对LR的好处？
 LR 适用于稀疏特征原因？
为什么LR不宜使用连续特征？
FM相比于直接做特征交叉的逻辑回归的好处？
FM相比于直接做特征交叉的逻辑回归的好处？
FM相比于直接做特征交叉的逻辑回归的好处？
FM的计算复杂度？
FFM怎么改进的FM，缺点是什么？
GBDT + LR 的优点和缺点？
Wide&Deep带L1正则化项的FTRL作为wide部分的优化方法
 item2vec的缺点？
冷启动思路？
召回和排序的优化目标设置？
模型训练一般是抽样的，这样的方式对哪些评价指标会有影响？
计算广告为什么要进行模型预估的ctr进行校准？
模型预估的CTR校准方法？
CTR计算公式修正（ctr平滑）
样本的展示位置对模型的影响
召回阶段正负样本采样策略，排序阶段采样策略
hard样本对召回阶段的影响
AB测试的p值的含义和一般有意义的P值
batch更新模型有什么弊端
怎么理解和解决推荐系统越推越窄的问题