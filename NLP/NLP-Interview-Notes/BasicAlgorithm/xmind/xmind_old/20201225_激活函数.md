### 激活函数ReLu有什么优缺点？ReLu的问题是什么？为什么要激活函数？而且还是非线性的激活函数？没了激活函数会怎样？

![激活函数](img/激活函数.png)

【动机：为什么要有激活函数？】

​    1.数据角度：由于数据是线性不可分的，如果采用线性化，那么需要复杂的线性组合去逼近问题，因此需要非线性变换对数据分布进行重新映射
​    2.线性模型的表达力问题：由于线性模型的表达能力不够，引入激活函数添加非线性因素

【介绍：激活函数有哪些？】

​    一、sigmoid：

$\sigma(x)=\frac {1}{1+e^{-x}}$

​        1.1 介绍：能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.

​        1.2 缺点：
​            1) 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。
​            2)output 不是0均值（即zero-centered）
​            3)幂函数耗时

​    二、tanh：

$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

​        2.1 特点：解决 非 0 均值 问题
​        2.2 缺点
​               1) 梯度爆炸和梯度消失
​                2) 幂函数耗时

​    三、relu：

$f(x)=max(0, x)$

​        3.1 特点
​            1) 解决了gradient vanishing问题 (在正区间)
​            2) 计算速度非常快，只需要判断输入是否大于0
​            3) 收敛速度远快于sigmoid和tanh
​        3.2 缺点：
​            1) ReLU的输出不是zero-centered
​            2) Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。

【使用：应用中如何选择合适的激活函数？】

​    1）深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度；
​    2）如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.
​    3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.



