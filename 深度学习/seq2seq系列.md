## æœºå™¨ç¿»è¯‘ä¸æ•°æ®é›†

æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œéƒ½æ˜¯åˆ¶è¡¨ç¬¦åˆ†éš”æ–‡æœ¬åºåˆ—å¯¹ï¼Œåºåˆ—å¯¹ç”±è‹±æ–‡æ–‡æœ¬åºåˆ—å’Œç¿»è¯‘åçš„æ³•è¯­æ–‡æœ¬åºåˆ—ç»„æˆã€‚æ¯ä¸ªæ–‡æœ¬åºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­ï¼Œä¹Ÿå¯ä»¥æ˜¯åŒ…å«å¤šä¸ªå¥å­çš„ä¸€ä¸ªæ®µè½ã€‚è‹±è¯­æ˜¯ *æºè¯­è¨€*ï¼ˆsource languageï¼‰ï¼Œæ³•è¯­æ˜¯ *ç›®æ ‡è¯­è¨€*ï¼ˆtarget languageï¼‰ã€‚

### è·å–æ•°æ®

```python
import os
import torch
from d2l import torch as d2l

def read_data_nmt():
    """è½½å…¥â€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®é›†ã€‚"""
    data_dir = d2l.download_extract('fra-eng')
    with open(os.path.join(data_dir, 'fra.txt'), 'r', 
             encoding='utf-8') as f:
        return f.read()
raw_text = read_data_nmt()
print(raw_text[:75])
#Go.	Va !
#Hi.	Salut !
#Run!	Coursâ€¯!
#Run!	Courezâ€¯!
#Who?	Qui ?
#Wow!	Ã‡a alorsâ€¯!
```

### æ–‡æœ¬é¢„å¤„ç†

å°†å¤§å†™å­—æ¯æ›¿æ¢æˆå°å†™ï¼Œåœ¨å•è¯å’Œæ ‡ç‚¹ä¹‹é—´ç©ºæ ¼

```python
def preprocess_nmt(text):
    def no_space(char,pre_char): 
        # è¿™ä¸ªå‡½æ•°æ˜¯è¾“å…¥å­—ç¬¦å’Œè¯¥å­—ç¬¦çš„å‰ä¸€ä¸ªå­—ç¬¦ï¼Œå¦‚æœè¯¥å­—ç¬¦æ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œ
        #ä¸”å‰ä¸€ç¬¦å·æ²¡æœ‰ç©ºæ ¼ï¼Œå°±è¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚
        return char in set(',.!?') and pre_char != ' '
    text = text.replace('\t202f',' ').replace('\xa0',' ').lower() 
    # utf-8ä¸­åŠè§’å…¨è§’éƒ½æ›¿æ¢æˆç©ºæ ¼ï¼Œå¹¶ä¸”å¤§å†™è½¬æ¢æˆå°å†™
    out = [' '+char if i > 0 and no_space(char,text[i-1]) else char
          for i, char in enumerate(text)]
    return ''.join(out)
text = process_nmt(raw_text)
```

### è¯å…ƒåŒ–

```python
def tokenize_nmt(text,num_examples=None):
    source,target=[],[]
    for i,line in enumerate(text.split('\n')):
        if num_examples and i > num_examples:
            break
        parts = line.split('\t') # å°†æ¯æ¡æ ·æœ¬åˆ’åˆ†æˆsourceå’Œtarget
        if len(parts) == 2:
            source.append(parts[0].split(' '))# å–å‡ºsourceï¼Œå†ä»¥ç©ºæ ¼åˆ†æˆè¯å…ƒ
            target.append(parts[1].split(' '))
    return source,target
source, target = tokenize_nmt(text)
```

### æ„å»ºè¯æ±‡è¡¨

ä¸ºç¼“è§£è¯æ±‡è¡¨è¿‡å¤§ï¼Œå°†å‡ºç°æ¬¡æ•°å°‘äº2æ¬¡çš„ä½é¢‘ç‡è¯å…ƒè§†ä¸ºç›¸åŒçš„æœªçŸ¥ï¼ˆâ€œ<unk>â€ï¼‰è¯å…ƒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æŒ‡å®šäº†é¢å¤–çš„ç‰¹å®šè¯å…ƒï¼Œä¾‹å¦‚åœ¨å°æ‰¹é‡æ—¶ç”¨äºå°†åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦çš„å¡«å……è¯å…ƒï¼ˆâ€œ<pad>â€ï¼‰ï¼Œä»¥åŠåºåˆ—çš„å¼€å§‹è¯å…ƒï¼ˆâ€œ<bos>â€ï¼‰å’Œç»“æŸè¯å…ƒï¼ˆâ€œ<eos>â€ï¼‰

```python
src_vocab = d2l.Vocab(source,min_freq=2,
                      reversed_token=['<pad>', '<bos>', '<eos>'])
```

### åŠ è½½æ•°æ®é›†

ç‰¹å®šçš„â€œ<eos>â€è¯å…ƒæ·»åŠ åˆ°æ‰€æœ‰åºåˆ—çš„æœ«å°¾ï¼Œç”¨äºè¡¨ç¤ºåºåˆ—çš„ç»“æŸã€‚è¿˜è®°å½•äº†æ¯ä¸ªæ–‡æœ¬åºåˆ—çš„é•¿åº¦

```python
# è¯¥å‡½æ•°è¾“å…¥ä¸€ä¸ªåºåˆ—æ ·æœ¬ï¼Œç„¶åè¿”å›åšå¥½å¡«å……æˆ–æˆªæ–­çš„åºåˆ—æ ·æœ¬
def truncate_pad(line,num_steps,padding_token):
    if len(line)>num_steps:
        return line[:num_steps]
    return line+[padding_token]*(num_steps-len(line)) #å¡«å……

def build_array_nmt(lines,vocab,num_steps):
    lines = [vocab[l] for l in lines]
    lines = [l+[vocab['<eos>']] for l in lines]
    array = torch.tensor([truncate_pad(
        l, num_steps, vocab['<pad>']) for l in lines])
    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
    return array,valid_len
```

### è®­ç»ƒæ¨¡å‹

**å°±æ˜¯ä¼ å…¥batch_sizeï¼Œnum_stepsç­‰å‚æ•°ï¼Œæ„å»ºä¸€ä¸ªæ•°æ®è¿­ä»£å™¨ã€‚è¯¥å‡½æ•°å…ˆè·å–åŸå§‹æ–‡æœ¬ï¼Œç„¶åå¯¹æ–‡æœ¬åšé¢„å¤„ç†ï¼Œå¤„ç†å®ŒåæŠŠæ–‡æœ¬è¯å…ƒåŒ–ï¼Œç„¶åé€åˆ°ä¹‹å‰å†™å¥½çš„è¯æ±‡è¡¨æ„å»ºå‡½æ•°ï¼ˆè¯¥å‡½æ•°éœ€è¦è¯å…ƒåŒ–çš„æ–‡æœ¬ï¼Œéœ€è¦æ–°æ·»åŠ çš„è¯å…ƒåˆ—è¡¨ç­‰ï¼‰ï¼Œç„¶åå°±è·å¾—äº†æ–‡æœ¬çš„è¯æ±‡è¡¨ï¼Œå°±å¯ä»¥å°†æ–‡æœ¬å†å¤„ç†ï¼Œä½¿å…¶æ¯ä¸ªåºåˆ—æ–‡æœ¬çš„æ—¶é—´æ­¥ç»Ÿä¸€ï¼ˆè¿™é‡Œå°†æ¯ä¸ªå¥å­ä½œä¸ºåºåˆ—æ ·æœ¬ï¼Œå› æ­¤éœ€è¦æˆªæ–­æˆ–å¡«å……ï¼Œä½¿å…¶é•¿åº¦å¯¹é½ï¼‰ã€‚ç„¶åå°†åºåˆ—é•¿åº¦ç»Ÿä¸€å¥½çš„æ–‡æœ¬ï¼Œè¾“å…¥åˆ°ä¹‹å‰å†™å¥½çš„æ•°æ®è¿­ä»£å™¨å‡½æ•°ï¼ˆè¯¥å‡½æ•°éœ€è¦ä¼ å…¥å¯åˆ†æ‰¹çš„æ–‡æœ¬ä»¥åŠbatch_sizeï¼‰ï¼Œæœ€åè·å¾—è¾“å‡ºã€‚**

```python
def load_data_nmt(batch_size,num_steps,num_examples=600):
    text = preprocess_num(read_data_nmt())
    source,target = tokenize_nmt(text,num_examples)
    src_vocab = d2l.Vocab(source,min_freq=2,
                      reversed_token=['<pad>', '<bos>', '<eos>'])
    tgt_vocab = d2l.Vocab(target,min_freq=2,
                      reversed_token=['<pad>', '<bos>', '<eos>'])
    src_arrays,src_valid_len = build_array_nmt(source,src_vocab,num_steps)
    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
    data_iter = d2l.load_array(data_arrays, batch_size)
    return data_iter,src_vocab,tgt_vocab
```



## ç¼–ç å™¨-è§£ç å™¨æ¶æ„

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211014155434202.png" alt="image-20211014155434202" style="zoom:67%;" />

### ç¼–ç å™¨

```python
from torch import nn
class Encoder(nn.Module):
    def __init__(self,**kwargs):
        super(Encoder,self).__init__(**kwargs)
    def forward(self,X,*args):
        raise NotImplementedError
```

### è§£ç å™¨

```python
class Decoder(nn.Module):
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)
    def ini_state(self,enc_outputs,*args):
        raise NotImplementedError
    def forward(self, X, state):
        raise NotImplementedError
```

### åˆå¹¶ç¼–ç å™¨å’Œè§£ç å™¨

```python
class EncoderDecoder(nn.Module):
    def __init__(self,encoder,decoder,**kwargs):
        super(EncoderDecoder,self).__init__(**kwargs)
        self.encoder = Encoder
        self.decoder = Decoder
    def forward(self,enc_X,dec_X,*args):
        enc_outputs = self.encoder(enc_X,*args)
        dec_state = self.decoder.init_state(enc_outputs,*args)
        return self.decoder(dec_X,dec_state)
```



## seq2seqæ¨¡å‹

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211014155847402.png" alt="image-20211014155847402" style="zoom: 80%;" />

### ç¼–ç å™¨

ç¼–ç å™¨ç”¨nnä¸­çš„Embeddingå±‚æ¥å°†è¯è¡¨ä¸­çš„è¯å…ƒè½¬æ¢ä¸ºç‰¹å¾å‘é‡ï¼ˆç±»ä¼¼äºå‰é¢ç”¨åˆ°çš„one-hotï¼‰ï¼Œè¿™é‡Œç”¨äº†ä¸¤å±‚çš„gruæ¥åšç¼–ç å™¨ï¼Œç¼–ç å™¨çš„å‰å‘ä¼ æ’­è¾“å‡ºæœ‰ä¸¤ä¸ª

```python
import collections
import math
import torch
from torch import nn
from d2l import torch as d2l
class Seq2SeqEncoder(d2l.Encoder):
    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,
                dropout=0,**kwargs):
        self.embedding = nn.Embedding(vocab_size,embed_size)
        self.rnn = nn.GRU(embed_size,num_hiddens,num_layers,dropout=dropout)
    def forward(self,X,*args):
        # è¾“å…¥Xçš„å½¢çŠ¶å˜ä¸º(batch_size,num_steps,embed_size)
        X = self.embedding(X)
        # åœ¨rnnæ¨¡å‹ä¸­åšç»´åº¦äº¤æ¢ï¼ŒæŠŠæ—¶é—´æ­¥æ”¾åœ¨ç¬¬ä¸€ç»´
        X = X.permute(1,0,2)
        output,state = self.rnn(X)
        # `output`çš„å½¢çŠ¶: (`num_steps`, `batch_size`, `num_hiddens`)
        # `state`çš„å½¢çŠ¶: (`num_layers`, `batch_size`, `num_hiddens`)
        return output,state
```

### è§£ç å™¨

ç›´æ¥ä½¿ç”¨ç¼–ç å™¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ¥åˆå§‹åŒ–è§£ç å™¨çš„éšè—çŠ¶æ€ã€‚è¿™å°±è¦æ±‚ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œå®ç°çš„ç¼–ç å™¨å’Œè§£ç å™¨å…·æœ‰ç›¸åŒæ•°é‡çš„å±‚å’Œéšè—å•å…ƒ

```python
class Seq2SeqDecoder(d2l.Decoder):
    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,
                 dropout=0,**kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size,embed_size)
        self.rnn = nn.GRU(embed_size+num_hiddens,num_hiddens,
                          num_layers,dropout=dropout)
        self.dense = nn.Linear(num_hiddens,vocab_size) #è·å–è¾“å‡ºç»“æœ
    def ini_state(self,enc_outputs,*args):
        return enc_outputs[1] # ç”¨ç¼–ç å™¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰å±‚çš„éšå±‚çŠ¶æ€æ¥åˆå§‹åŒ–
    def forward(self, X, state):
        # æ­¤æ—¶Xçš„å½¢çŠ¶ä¸ºnum_step,batch_size,embeding_size
        X = self.embedding(X).permute(1,0,2) 
        context = state[-1].repeat(X[0],1,1)
        # state[-1]çš„å½¢çŠ¶(1, `batch_size`, `num_hiddens`)
        # contextçš„å½¢çŠ¶(num_step, `batch_size`, `num_hiddens`)
        #å°†contextå’ŒXåœ¨ç¬¬ä¸‰ç»´æ‹¼æ¥
        #æœ€åå°±æ˜¯(num_step, batch_size, num_hiddens+embed_size)
        X_and_context = torch.concat((X,context),2)
        output,state = self.rnn(X_and_context)
        output = self.dense(output).premute(1,0,2)
        return output, state
    
#è§£ç å™¨çš„è¾“å‡ºå½¢çŠ¶å˜ä¸ºï¼ˆæ‰¹é‡å¤§å°, æ—¶é—´æ­¥æ•°, è¯è¡¨å¤§å°)ï¼Œå…¶ä¸­å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦å­˜å‚¨é¢„æµ‹çš„è¯å…ƒåˆ†å¸ƒã€‚
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
decoder.eval()
state = decoder.init_state(encoder(X))
output, state = decoder(X, state)
output.shape, state.shape
#(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))
```

### æŸå¤±å‡½æ•°

åº”è¯¥å°†å¡«å……è¯å…ƒçš„é¢„æµ‹æ’é™¤åœ¨æŸå¤±å‡½æ•°çš„è®¡ç®—ä¹‹å¤–ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨ä¸‹é¢çš„ `sequence_mask` å‡½æ•°[**é€šè¿‡é›¶å€¼åŒ–å±è”½ä¸ç›¸å…³çš„é¡¹**]ï¼Œä»¥ä¾¿åé¢ä»»ä½•ä¸ç›¸å…³é¢„æµ‹çš„è®¡ç®—éƒ½æ˜¯ä¸é›¶çš„ä¹˜ç§¯ï¼Œç»“æœéƒ½ç­‰äºé›¶ã€‚

```python
def sequence_mask(X,valid_len,value=0):
    maxlen = X.size(1) # è·å–æ—¶é—´æ­¥å¤§å°
    # ä¸‹é¢è¿™é‡Œå…ˆåšäº†å‡ç»´ç„¶åç”¨çš„å¹¿æ’­æœºåˆ¶
    mask = torch.arange((maxlen), dtype=torch.float32,
                        device=X.device)[None, :] < valid_len[:, None]
    X[~mask] = value
    return X
# [:, None]æ˜¯å‡ä¸€ä¸ªç»´åº¦çš„æ“ä½œ,åœ¨noneå‡ºç°çš„ä½ç½®å‡ä¸€ç»´
#mask, torch.arange((maxlen), valid_len[:, None]ä¸‰ä¸ªçš„å€¼åˆ†åˆ«æ˜¯ä»¥ä¸‹çš„æ ·å­
#(tensor([[ True, False, False],
#        [ True,  True, False]]),
# tensor([0., 1., 2.]),
# tensor([[1],
#        [2]]))
```

ç°åœ¨ï¼Œå¯ä»¥**é€šè¿‡æ‰©å±•softmaxäº¤å‰ç†µæŸå¤±å‡½æ•°æ¥é®è”½ä¸ç›¸å…³çš„é¢„æµ‹**

```python
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    def forward(self,pred,label,valid_len):
        # `pred` çš„å½¢çŠ¶ï¼š(`batch_size`, `num_steps`, `vocab_size`)
    	# `label` çš„å½¢çŠ¶ï¼š(`batch_size`, `num_steps`)
   	 	# `valid_len` çš„å½¢çŠ¶ï¼š(`batch_size`,)
        #å…ˆç”Ÿæˆä¸€ä¸ªå…¨1çŸ©é˜µï¼Œç„¶åç”¨ä¸Šé¢çš„æ–¹æ³•ï¼ŒæŠŠå¡«å……éƒ¨åˆ†çš„æƒé‡è®¾ä¸º0
        weights = torch.ones_like(label)
        weights = sequence_mask(weights,valid_len)
        self.reduction='none'
        # pytorchéœ€è¦æŠŠé¢„æµ‹çš„ç»´åº¦æ”¾åœ¨ä¸­é—´ï¼Œå› æ­¤å…ˆåšä¸€ä¸ªç»´åº¦äº¤æ¢
        unweight_loss = super().forward(pred.permute(0,2,1),label)
        weighted_loss = (unweight_loss*weights).mean(dim=1)
        return weighted_loss
```

### è®­ç»ƒ

**ç‰¹å®šçš„åºåˆ—å¼€å§‹è¯å…ƒï¼ˆâ€œ<bos> â€ï¼‰å’ŒåŸå§‹çš„è¾“å‡ºåºåˆ—ï¼ˆä¸åŒ…æ‹¬åºåˆ—ç»“æŸè¯å…ƒâ€œ<eos>â€ï¼‰æ‹¼æ¥åœ¨ä¸€èµ·ä½œä¸ºè§£ç å™¨çš„è¾“å…¥**ï¼Œè¿™è¢«ç§°ä¸º *æ•™å¸ˆå¼ºåˆ¶ï¼ˆteacher forcingï¼‰ï¼Œå› ä¸ºåŸå§‹çš„è¾“å‡ºåºåˆ—ï¼ˆè¯å…ƒçš„æ ‡ç­¾ï¼‰è¢«é€å…¥è§£ç å™¨ã€‚æˆ–è€…ä¹Ÿå¯ä»¥å°†æ¥è‡ª**ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„ é¢„æµ‹ å¾—åˆ°çš„è¯å…ƒä½œä¸ºè§£ç å™¨çš„å½“å‰è¾“å…¥ã€‚**

```python
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)
    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = MaskedSoftmaxCELoss()
    net.train()
    # ä¸Šé¢è¿™äº›éƒ½æ˜¯ä¸€æ ·çš„æ“ä½œï¼šå‚æ•°åˆå§‹åŒ–ï¼Œæ¨¡å‹ç§»åˆ°gpuï¼Œå®šä¹‰ä¼˜åŒ–å™¨ã€æŸå¤±
    for epoch in range(num_epochs):
        timer = d2l.Timer()
        metirc = d2l.Accumulator(2) # è®­ç»ƒæŸå¤±æ€»å’Œï¼Œè¯å…ƒæ•°é‡
        for batch in data_iter:
            X,X_valid_len,Y,Y_valid_len = [x.to(device) for x in batch]
            #tgt_vocab['<bos>']*Y.shape[0]å…ˆå¤åˆ¶batch_sizeä¸ª<bos>
            #ç„¶ååœ¨reshapeæˆåˆ—æ•°ä¸º1ï¼Œæ–¹ä¾¿æ‹¼æ¥ï¼ˆä¹Ÿå°±æ˜¯å¤§å°ä¸º(batch_size,1)ï¼‰
            bos = torch.tensor([tgt_vocab['<bos>']*Y.shape[0],
                                device=device]).reshape(-1,1)
            # åœ¨å“ªä¸ªç»´åº¦ä¸Šæ‹¼æ¥ï¼Œå°±æ˜¯æŠŠä¸¤ä¸ªçŸ©é˜µè¿™ä¸ªç»´åº¦ä¸Šçš„æ•°è¿›è¡Œç›¸åŠ 
            dec_input = torch.concat([bos,Y[:,:-1]],1)
            #EncoderDecoderåšå‰å‘çš„æ—¶å€™å°±æ˜¯éœ€è¦ç¼–ç å’Œè§£ç çš„è¾“å…¥æ•°æ®ï¼Œå…¶ä»–çš„å¦‚æ ¹æ®æƒ…å†µæ¥å®š
            #å¦‚X_valid_lenåœ¨EncoderDecoderæ—¶éœ€è¦ï¼Œè€ŒY_valid_lenåœ¨ç®—lossçš„æ—¶å€™æ‰éœ€è¦
            Y_hat,_ = net(X,dec_input,X_valid_len)
            l = loss(Y_hat, Y, Y_valid_len)
            l.sum().backward()
            d2l.grad_clipping(net, 1)
            num_tokens = Y_valid_len.sum()
            optimizer.step()
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')

embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = d2l.EncoderDecoder(encoder, decoder)
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

### é¢„æµ‹

ä¸ºäº†é‡‡ç”¨ä¸€ä¸ªæ¥ç€ä¸€ä¸ªè¯å…ƒçš„æ–¹å¼é¢„æµ‹è¾“å‡ºåºåˆ—ï¼Œæ¯ä¸ªè§£ç å™¨å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥éƒ½å°†æ¥è‡ªäºå‰ä¸€æ—¶é—´æ­¥çš„é¢„æµ‹è¯å…ƒã€‚åºåˆ—å¼€å§‹è¯å…ƒï¼ˆâ€œ<bos>â€ï¼‰åœ¨åˆå§‹æ—¶é—´æ­¥è¢«è¾“å…¥åˆ°è§£ç å™¨ä¸­ã€‚å½“è¾“å‡ºåºåˆ—çš„é¢„æµ‹é‡åˆ°åºåˆ—ç»“æŸè¯å…ƒï¼ˆâ€œ<eos>â€ï¼‰æ—¶ï¼Œé¢„æµ‹å°±ç»“æŸäº†ã€‚

```python
def predict_seq2seq(net,src_sentence,src_vocab,tgt_vocab,num_steps
                   device, save_attention_weights=False):
    net.eval()
    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
        src_vocab['<eos>']] # å°†åŸå§‹å¥å­åˆ’åˆ†æˆè¯å…ƒå¹¶åŠ ä¸Š<eos>
    # è®¡ç®—å‡ºvalid_lenå¹¶åšå¡«å……æˆ–è£å‰ªï¼Œå’Œå‰é¢è®­ç»ƒçš„å·®ä¸å¤š
    enc_valid_len = torch.tensor([len(src_tokens)], device=device)
    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
    # æ·»åŠ æ‰¹é‡è½´
    enc_X = torch.unsqueeze(
        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
    #unsqueezeæ—¶æ·»åŠ ç»´åº¦ï¼Œè¿™é‡Œæ˜¯å¯¹å¼ é‡åœ¨ç¬¬0ç»´åŠ ä¸ªç»´åº¦ï¼Œå³batchçš„ç»´åº¦
    enc_outputs = net.encoder(enc_X, enc_valid_len)
    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
    # æ·»åŠ æ‰¹é‡è½´
    dec_X = torch.unsqueeze(torch.tensor(
        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
    output_seq, attention_weight_seq = [], []
    for _ in range(num_steps):
        Y,dec_state = net.decoder(dec_X,dec_state)
        dec_X = Y.argmax(dim=2) #Yçš„æœ€åä¸€ç»´å°±æ˜¯è¾“å‡ºçš„ç±»åˆ«ï¼Œè¿™ä¸€ç»´çš„å¤§å°æ˜¯vocab_size
        # è¿™é‡Œé‡‡ç”¨çš„æ˜¯è´ªå¿ƒæœç´¢æ³•ï¼Œå³å°†é¢„æµ‹æœ€é«˜å¯èƒ½æ€§çš„è¯å…ƒï¼Œä½œä¸ºè§£ç å™¨ä¸‹ä¸€æ—¶é—´æ­¥çš„è¾“å…¥
        pred = dec_X.squeeze(dim=0).type(torch.int32).item()
        #squeezeå°†è¾“å…¥å¼ é‡å½¢çŠ¶ä¸­dim=0ç»´çš„1 å»é™¤å¹¶è¿”å›ï¼Œå®ç°é™ç»´
    	# ä¿å­˜æ³¨æ„åŠ›æƒé‡ï¼ˆç¨åè®¨è®ºï¼‰
        if save_attention_weights:
            attention_weight_seq.append(net.decoder.attention_weights)
        if pred == tgt_vocab['<eos>']:
            break
        output_seq.append(pred)
    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq
```

### é¢„æµ‹åºåˆ—çš„è¯„ä¼°ï¼ˆBLEUï¼‰

bleuå®šä¹‰å¦‚ä¸‹ï¼š

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211014185518053.png" alt="image-20211014185518053" style="zoom: 67%;" />

å…¶ä¸­ ğ‘˜ æ˜¯ç”¨äºåŒ¹é…çš„æœ€é•¿çš„ ğ‘› å…ƒè¯­æ³•ã€‚

```python
def bleu(pred_seq, label_seq, k):
    """è®¡ç®— BLEU"""
    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    score = math.exp(min(0, 1 - len_label / len_pred))  # è®¡ç®—å‡ºå‰åŠéƒ¨åˆ†
    for n in range(1, k + 1):
        num_matches, label_subs = 0, collections.defaultdict(int)
        for i in range(len_label - n + 1):
            label_subs[''.join(label_tokens[i: i + n])] += 1
        for i in range(len_pred - n + 1):
            if label_subs[''.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[''.join(pred_tokens[i: i + n])] -= 1 #åŒ¹é…åˆ°å°±å‡å»ï¼Œé˜²æ­¢é‡å¤
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score
```



## æŸæœç´¢

æŸæœç´¢ä»‹äºç©·ä¸¾æœç´¢å’Œè´ªå¿ƒæœç´¢ä¹‹é—´ã€‚ç¬¬ä¸€æ¬¡é€‰å–å‰Kä¸ªå¤§çš„å€¼ï¼Œç„¶åä¸€ç›´å‘ä¸‹æœç´¢ï¼Œæœ€ç»ˆå¾—åˆ°kä¸ªç»“æœ

![image-20211014155543908](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211014155543908.png)

å‰é¢é¢„æµ‹çš„æ¨¡å‹ä¸­ï¼Œä½¿ç”¨çš„æ˜¯è´ªå¿ƒï¼Œé€Ÿåº¦å¿«ï¼Œä½†å‡†ç¡®ä½ã€‚å¯ä»¥æ”¹ç”¨æŸæœç´¢ï¼Œåœ¨å‡†ç¡®åº¦å’Œé€Ÿåº¦ä¹‹é—´æœ‰ä¸ªæƒè¡¡ã€‚