

## 一、数据读取

手动定义数据迭代器，需要传入所有输入数据和标签，以及batch大小。每调用一次这个迭代函数，都会获取一个batch，直到所有数据遍历完。

```python
def data_iter(batch_size, features, labels):
    num_examples = len(labels)  # 总的数据样本数
    indices = list(range(num_examples))  # 生成一个索引
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        # 这里的range(0, num_examples, batch_size)是对0到num_examples生成的数据，按batch_size跳步进行获取数据（如果没有该参数则默认跳步为1）
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```



调用API实现数据读取，需要传入数据集、batch大小以及是否shuffle。

- 首先需要将原始的数据集（array等），转换成torch下使用的数据集，这里使用torch.utils.data中的TensorDataset类的一个对象（可以**根据需要定义一个TensorDataset类的子类**，来实现具体的方法）
- 第二步创建torch.utils.data下的DataLoader类的一个对象。用来实现数据的采样、转换和处理。

```python
from torch.utils import data
from torch import nn

def load_array(data_arrays, batch_size, is_train=True):
    dataset = data.TensorDataset(*data_arrays) # 在参数前加星号，来将传递进来的多个参数转化为一个对象、元组或字典（一个星号表示存放在一个元组中，两个星号表示存放在一个字典中）
    return data.DataLoader(dataset,batch_size,shuffle=is_train)

batch_size = 10
data_iter = load_array((features,labels),batch_size)
```



## 二、模型定义

对于模型的定义，使用torch.nn包中的Sequential类的对象来实现。该对象会按参数顺序来将各个层进行拼接。Flatten是展开层，将输入展开成1维。

```python
net = nn.Sequential(nn.Flatten(),nn.Linear(784,256),nn.ReLU(),nn.Linear(256,10))
```



### 1.自定义块

从零实现一个自定义块只需要提供自己的构造函数和正向传播函数即可。下面定义一个多层感知器的块：

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用`MLP`的父类`Block`的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```

### 2.自定义顺序块

前面的Sequential就是实现了顺序块的功能，现在自定义一个：

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        # 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量
        # `_modules` 中。`block`的类型是OrderedDict。
        for block in args:
            self._modules[block] = block
    def forward(self,X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
    	return X
```

### 3.块的灵活使用

块既可以嵌套使用，也可以在其中集成一些其他操作，例如控制流、矩阵运算等。



### 4.自定义层

层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用ReLU作为激活函数。该层需要输入参数：`in_units`和`units`，分别表示输入和输出的数量。

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```





## 三、模型参数

### 初始化模型参数

自定义权重初始化使用torch.randn、自定义偏置初始化使用torch.zeros。

```python
from torch import nn
num_inputs, num_outputs, num_hiddens = 784, 10, 256
# 法1
W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]

# 法2
net[0].weight.data.normal_(0,0.01) # 注意末尾要加上_符号
net[0].bias.data.fill_(0)
```

简洁实现方法：

```python
def init_weights(m):
    if type(m) == nn.Linear: # 先判断网络的模型
        nn.init.normal_(m.weight, std=0.01)
        #nn.init.constant_(m.weight,42) #初始化为常数
        #nn.init.xavier_uniform_(m.weight) #Xavier初始化
        nn.init.zeros_(bias)

net.apply(init_weights);	
```

> 如果初始化模型参数不当，会出现梯度爆炸，或者对称性等问题。

### 网络各层参数的访问

还可以通过以下方式来获取并初始化网络各层的参数：

```python
net[2].state_dict() # 查看该层的所有参数
print(*[(name, param.shape) for name, param in net.named_parameters()]) #一次性访问所有的参数
net.state_dict()['2.bias'].data # 根据名称来访问参数
```

### 嵌套块的参数访问

因为层是分层嵌套的，所以可以像通过嵌套列表索引一样来访问嵌套层的参数。

### 参数绑定

当希望多个层共享参数时，可以预先定义一个共享层，然后在sequential中多次引用该共享层。

### 读写文件

有时我们对所学的模型足够满意，我们希望保存训练的模型以备将来在各种环境中使用。

#### 加载和保存张量

```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file') # 写操作
# 可以读写张量列表、字典等
x2 = torch.load('x-file') # 读操作
```

#### 加载和保存模型参数

深度学习框架提供了内置函数来保存和加载整个网络。为了恢复模型，我们需要用代码生成结构，然后从磁盘加载参数。假设我们训练了一个叫MLP的模型，我们想保存某一次训练的结果：

```python
torch.save(net.state_dict(), 'mlp.params') # 保存模型参数

clone = MLP() # 先实例化一个模型
clone.load_state_dict(torch.load('mlp.params')) # j
```





## 四、损失函数

损失函数可以直接调用torch.nn包中定义好的损失函数，例如：

```python
nn.MSELoss()
nn.CrossEntropyLoss()
```



> 为避免模型过拟合，或梯度爆炸等问题，可以使用正则化技术，即在损失函数中加入正则化项（如L1范数、L2范数）



## 五、优化算法

优化算法可以自己定义，也可以调用torch.optim包中定义好的优化算法，例如：

```
updater = torch.optim.SGD(net.parameters(),lr=0.03)
```

优化算法主要是用来跟新模型参数，所以只需要传入模型参数和学习率即可。



## 六、训练过程

**（函数参考softmax回归从零实现，还有关于训练过程动画的函数定义等。）**

**整个训练过程是，先定义超参数（学习率、epoch大小、batch_size等），然后针对每个batch，先调用迭代器获取每个batch的数据子集存储成一个输入和一个输出，然后将输入放到初始化好的模型中做正向传播，此时获得预测值，再将预测值与真实值（输出）放到损失函数中，对该损失函数进行反向传播，然后再调用优化算法。直到遍历完所有的batch和epoch，结束训练过程。**

因为所有的训练过程都差不多，因此可以将训练过程进行封装。先封装一个训练单个迭代周期的函数。然后只需要传入自己定义好的模型、训练数据迭代器、损失函数、优化算法即可，函数返回训练损失（与输出shape相同的向量）、准确度和训练样本数量。

```python
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）。"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.backward()
            updater.step()
            metric.add(float(l) * len(y), accuracy(y_hat, y),
                       y.size().numel())
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练准确率
    return metric[0] / metric[2], metric[1] / metric[2]
```

最后封装一个完整的训练函数，需要传入的参数有自己定义好的模型、训练数据迭代器、测试数据迭代器、损失函数、优化算法、迭代周期。

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（定义见第3章）。"""
    #animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
    #                    legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        #animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```



## 七、测试过程

测试模型需要先定义一个评估正确率的函数：

```python
def evaluate_accuracy(net, data_iter):  #@save
    """计算在指定数据集上模型的精度。"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```



## 八、其他问题

### 权重衰退与dropout

### 模型初始化与数值稳定性

### 分布偏移问题

### 欠拟合、过拟合与模型选择