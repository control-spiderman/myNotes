### 加性注意力

用途：计算query和key的注意力分数，并将结果作为权值与value相乘得到输出

函数输入：

- query：(batch_size,query_num, num_hiddens)
- key：(batch_size,key_num, num_hiddens)
- values：(batch_size,value_num, values_dimension)
- valid_lens：一维时为batch_size，二维时为（batch_size，query_num）

函数输出：(batch_size, query_num, values_embedding_size)

#### 遮蔽softmax

用途：对query到的value做softmax，其中把value中填充的部分（不需要的部分）的给遮蔽了

输入：

- X：（batch_size,query_num,values_num）
- valid_len：一维时为batch_size，二维时为（batch_size，query_num）

输出：与X形状相同，是对最后一维计算好softmax的结果

##### 序列遮蔽

输入：

- X：第一维是样本个数，第二维是样本序列长度
- valid_len：与X的第一维长度相同的一个向量

输出，与输入X相同

## Bahdanau 注意力

带注意力机制的解码器

##### 模型结构

- embedding层：vocab_size、embedd_size
- attention层：num_hiddens、num_hiddens、num_hiddens
- rnn层：embedd_size、num_hiddens、num_layers
- 线性输出层：embedd_size、vocab_size

##### 隐层状态初始化

enc_output,enc_state,enc_valid_len
前向传播

输入X：形状为batch_size,num_steps,vocab_size

输入state：enc_output,enc_state,enc_valid_len
1.对输入的词元embedding编码，并将num__step放在第一维
2.以时间步做循环：
获取query，key，value，计算context
  并将contex与x拼接