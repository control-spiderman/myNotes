## æ³¨æ„åŠ›æœºåˆ¶ç»Ÿè®¡å­¦åŸºç¡€

é—®é¢˜å¼•å…¥ï¼šcnnä¸­çš„å·ç§¯å±‚æ˜¯ç”¨æ¥æå–ç‰¹å¾çš„ï¼Œä½†è¿™å±äºè¢«åŠ¨æå–ç‰¹å¾ï¼Œå°±æ˜¯ä»€ä¹ˆç‰¹å¾æ›´æ˜æ˜¾å°±æå–ä»€ä¹ˆï¼Œç½‘ç»œæœ¬èº«æ˜¯ä¸çŸ¥é“è‡ªå·±iæƒ³è¦ä»€ä¹ˆçš„ã€‚æ‰€ä»¥ç»Ÿè®¡å­¦ä¸Šè®¤ä¸ºåº”è¯¥å°†è‡ªèº«éœ€è¦è€ƒè™‘è¿›å»ï¼Œä¹Ÿå°±æ˜¯é€‰æ‹©æœ€åˆé€‚çš„ï¼Œè€Œä¸æ˜¯é€‰æ‹©æœ€æ˜æ˜¾çš„ã€‚

### Nadaraya-Watson æ ¸å›å½’

**éå‚æ•°æ³¨æ„åŠ›æ±‡èš**

è¯¥æ–¹æ³•ä¼šå¯¹ä¸è‡ªå·±æœ€ç›¸è¿‘çš„æ ·æœ¬ç»™è¾ƒé«˜çš„æƒå€¼ï¼Œè€Œè¾ƒè¿œçš„æ ·æœ¬ç»™å°½å¯èƒ½ä½çš„æƒå€¼

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015142054872.png" alt="image-20211015142054872" style="zoom: 67%;" />

**å¸¦å‚æ•°æ³¨æ„åŠ›æ±‡èš**

æ ¸å‡½æ•°é€‰æ‹©é«˜æ–¯æ ¸å‡½æ•°

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015142215096.png" alt="image-20211015142215096" style="zoom:67%;" />



### æ‰¹é‡çŸ©é˜µä¹˜æ³•

**å‡å®šä¸¤ä¸ªå¼ é‡çš„å½¢çŠ¶åˆ†åˆ«æ˜¯ (ğ‘›,ğ‘,ğ‘) å’Œ (ğ‘›,ğ‘,ğ‘) ï¼Œå®ƒä»¬çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•è¾“å‡ºçš„å½¢çŠ¶ä¸º (ğ‘›,ğ‘,ğ‘)**

```python
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape
#torch.Size([2, 1, 6])
```

åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„èƒŒæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥[**ä½¿ç”¨å°æ‰¹é‡çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—å°æ‰¹é‡æ•°æ®ä¸­çš„åŠ æƒå¹³å‡å€¼**]ã€‚

```python
weights = torch.ones((2, 10)) * 0.1
values = torch.arange(20.0).reshape((2, 10))
#ä¸‹é¢è¿™ä¸€æ­¥å‡ç»´çš„æ“ä½œï¼šweightsä»(2ï¼Œ10)->(2,1,10)  valuesä»(2,10) -> (2,10,1)
#æ‰¹é‡çŸ©é˜µè®¡ç®—åå˜æˆ(2,1,1)
torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))
#tensor([[[ 4.5000]],
#        [[14.5000]]])
```



## æ³¨æ„åŠ›åˆ†æ•°

æ³¨æ„åŠ›åˆ†æ•°ä¹Ÿå°±æ˜¯ä¸‹é¢æ³¨æ„åŠ›æ±‡èšå±‚ä¸­çš„aï¼Œç°åœ¨è€ƒè™‘æˆå‘é‡

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015142952185.png" alt="image-20211015142952185" style="zoom:80%;" />

å…¶ä¸­ï¼š

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015143115909.png" alt="image-20211015143115909" style="zoom:67%;" />

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015143200139.png" alt="image-20211015143200139" style="zoom:67%;" />

### é®è”½softmaxæ“ä½œ

å‰é¢çš„softmaxæ“ä½œæ˜¯queryå¯¹æ‰€æœ‰çš„keyéƒ½æ“ä½œä¸€éï¼Œä½†å¾ˆå¤šæ—¶å€™æœ‰äº›keyæ˜¯ä¸éœ€è¦è®¡ç®—æƒé‡çš„ï¼Œå°±æ¯”å¦‚è¯´å‰é¢çš„ä¸ºäº†ä¿è¯æ¯ä¸ªæ ·æœ¬çš„æ—¶é—´æ­¥ä¸€æ ·ï¼Œå¯¹å¥å­åšäº†å¡«å……å’Œè£å‰ªã€‚é‚£ä¹ˆè¿™äº›å¡«å……éƒ¨åˆ†çš„æ—¶é—´æ­¥å°±æ˜¯é‚£äº›ä¸éœ€è¦ç®—æƒé‡çš„keyã€‚å› æ­¤è¦æŠŠä»–ä»¬ç»™maskäº†

```python
def masked_softmax(X,valid_len):
    # è¿™é‡ŒXçš„å½¢çŠ¶æ˜¯ï¼ˆbatch_size,query_num,key_numï¼‰ valid_lenæ˜¯ä¸€ç»´æˆ–äºŒç»´
    if valid_len is None:
        return nn.functional.softmax(X,dim=-1)
    else:
        shape = X.shape
        if valid_len.dim() == 1:
            valid_len = torch.repeat_interleave(valid_len,shape[1])
        else:
            valid_len = valid_len.reshape(-1)
        #sequence_maskä¸­çš„è¾“å…¥æ˜¯äºŒç»´çš„ï¼Œæ ·æœ¬æ•°Xæ—¶é—´æ­¥é•¿åº¦
        X = d2l.sequence_mask(X.reshape(-1,shape[-1]),valid_len,value=-1e6)
        return nn.functional.softmax(X.reshape(shape,dim=-1)
```



### åŠ æ€§æ³¨æ„åŠ›

åŠ æ€§æ³¨æ„åŠ›å°±ç›¸å½“äºå°†Qå’ŒKæ‹¼æ¥èµ·æ¥æ”¾åˆ°ä¸€ä¸ªéšè—å±‚å¤§å°ä¸ºhã€è¾“å‡ºå±‚ä¸º1çš„MLPä¸­ï¼Œéšè—å±‚æ¿€æ´»å‡½æ•°ä½¿ç”¨tanhï¼ŒQåˆ°éšå±‚çš„æƒé‡å°±æ˜¯Wqï¼ˆå¤§å°ä¸ºh,qï¼‰ï¼ŒKåˆ°éšå±‚çš„æƒé‡å°±æ˜¯Wkï¼ˆå¤§å°ä¸ºh,kï¼‰ï¼Œéšå±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ä¸ºWv (å¤§å°ä¸ºh)

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015143347547.png" alt="image-20211015143347547" style="zoom:67%;" />

```python
class AdditiveAttention(nn.Module):
    def __init__(self,key_Size,query_size,num_hiddens,dorpout,**kwargs):
        super().__init__(**kwargs)
        self.W_q = nn.Linear(query_size,num_hiddens,bias=False)
        self.W_k = nn.Linear(key_Size,num_hiddens,bias=False)
        self.W_v = nn.Linear(num_hiddens,1,bias=False)
    def forward(self,query,key,values,valid_lens):
        # querysçš„å½¢çŠ¶ï¼š(batch_size,query_num,num_hiddens)
        # keysçš„å½¢çŠ¶ï¼š(batch_size,key_num,num_hiddens)
        querys,keys = self.W_q(query),self.W_k(key)
        #featuresçš„å½¢çŠ¶ï¼š(batch_size,query_numï¼Œkey_num,num_hiddens)
        features = querys.unsqueeze(2)+keys.unsqueeze(1)
        features = torch.tanh(features)
        #scoresï¼š(batch_size,query_numï¼Œkey_num),æ¯ä¸ªæ•°éƒ½æ˜¯ä¸€ä¸ªscore
        scores = self.W_v(features).squeeze(-1)
        self.attention_weight = d2l.masked_softmax(score,valid_lens)
        #valueå½¢çŠ¶ä¸º(batch_size,value_num,embedding_size)
        #è¯¥å‡½æ•°è¾“å‡ºå½¢çŠ¶ä¸ºï¼š(batch_size,query_num,embedding_size)
        return torch.bmm(self.attention_weight,values)
```

### ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

ä½¿ç”¨ç‚¹ç§¯å¯ä»¥å¾—åˆ°è®¡ç®—æ•ˆç‡æ›´é«˜çš„è¯„åˆ†å‡½æ•°ã€‚ä½†æ˜¯ç‚¹ç§¯æ“ä½œè¦æ±‚æŸ¥è¯¢å’Œé”®å…·æœ‰ç›¸åŒçš„é•¿åº¦ ğ‘‘

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015161248648.png" alt="image-20211015161248648" style="zoom:67%;" />

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015161304048-16388789401353.png" alt="image-20211015161304048" style="zoom:67%;" />

```python
class DotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # è®¾ç½® `transpose_b=True` ä¸ºäº†äº¤æ¢ `keys` çš„æœ€åä¸¤ä¸ªç»´åº¦
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
```



## attentionæ“ä½œ



## åœ¨seq2seqæ¨¡å‹ä¸Šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015161450119.png" alt="image-20211015161450119" style="zoom:67%;" />

### Bahdanau æ³¨æ„åŠ›

å°†æ³¨æ„åŠ›æœºåˆ¶ç”¨åˆ°seq2seqæ¨¡å‹å«åšBahdanau æ³¨æ„åŠ›ã€‚è¿™ä¸ªæ¨¡å‹åªéœ€è¦ä¿®æ”¹seq2seqçš„è§£ç å™¨ã€‚

**å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨åŸºæœ¬æ¥å£**

```python
class AttentionDecoder(d2l.Decoder):
    """å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨åŸºæœ¬æ¥å£"""
    def __init__(self, **kwargs):
        super(AttentionDecoder, self).__init__(**kwargs)
    @property
    def attention_weights(self):
        raise NotImplementedError
```

**å®ç°å¸¦æœ‰Bahdanauæ³¨æ„åŠ›çš„å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨**

```python
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self,vocab_size,embedding_size,num_hiddens,
                 num_layers,dropout,**kwargs):
        super().__init__(**kwargs)
        self.attention=d2l.AdditiveAttention(num_hiddens,num_hiddens,
                                             num_hiddens,dropout)
        self.embedding=nn.Embedding(vocab_size,embedding_size)
        self.rnn=nn.GRU(embedding_size+num_hiddens,num_hiddens,num_layers,
                        dropout=dropout)
        self.dense=nn.Linear(num_hiddens,vocab_size)
        
    def init_state(self,enc_outputs,enc_valid_len,*args):
        outputs,hidden_state = enc_outputs
        return (outputs.permute(1,0,2),hidden_state,enc_valid_len)
    def forward(self,X,state):
        #enc_outputså½¢çŠ¶ä¸ºï¼šbatch_size,num_steps,num_hiddens
        #hidden_stateå½¢çŠ¶ä¸ºï¼šbatch_size,num_layers,num_hiddens
        #enc_valid_lenå½¢çŠ¶ä¸ºï¼šbatch_size
        enc_outputs,hidden_state,enc_valid_len = state
        X = self.embedding(X).permite(1,0,2)
        outputs,self._attention_weights = [],[]
        # Xçš„å½¢çŠ¶ (`num_steps`, `batch_size`, `embed_size`)
        for x in X:
            query = torch.unsqueeze(hidden_state[-1],dim=-1)
            context = self.attention(query,enc_outputs,enc_outputs,enc_valid_len)
            x = torch.cat((context,torch.unsqueeze(x,dim=1)),dim=-1)
            out,hidden_state = self.rnn(x.permute(1,0,2),hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        outputs = self.dense(torch.cat(outputs),dim=0)
        return outputs.permute(1,0,2),[enc_outputs,hidden_state,enc_valid_len]
            
    @property 
    def attention_weight(self):
        return self._attention_weights
```

**è®­ç»ƒ**

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 250, d2l.try_gpu()

train_iter,src_vocab,tgt_vocab = d2l.load_data_nmt(batch_size,num_steps)
encoder = d2l.Seq2SeqEncoder(
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net,train_iter,num_epoch,tgt_vocab,device)
```



## å¤šå¤´æ³¨æ„åŠ›

å¤šå¤´å°±ç›¸å½“äºCNNä¸­çš„å¤šé€šé“ï¼ˆæ¯ä¸ªé€šé“æå–ç‰¹ç‚¹çš„æ¨¡å¼ï¼‰ï¼Œè¿™é‡Œæ¯ä¸ªå¤´éƒ½åœ¨æ–‡æœ¬å…¨å±€ä¸­æå–ç‰¹å®šçš„ä¿¡æ¯ï¼ˆæ¨¡å¼ï¼‰ã€‚æ“ä½œæ–¹æ³•å°±æ˜¯æ¯ä¸ªå¤´éƒ½å°†è¾“å…¥æ•°æ®é€šè¿‡å…¨è¿æ¥å±‚çº¿æ€§å˜æ¢æŠ•å½±åˆ°ç‰¹å®šç»´åº¦çš„ç©ºé—´ä¸­ï¼Œç„¶åæå–åœ¨è¯¥ç‰¹å¾ç©ºé—´ä¸­çš„ä¿¡æ¯ã€‚

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015190433700-16388787848962.png" alt="image-20211015190433700" style="zoom:67%;" />

```python
#@save
class MultiHeadAttention(nn.Module):
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        output = self.attention(queries, keys, values, valid_lens)

        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
```



## è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç 

### è‡ªæ³¨æ„åŠ›

è‡ªæ³¨æ„åŠ›å°±æ˜¯å¯¹äºä¸€ä¸ªç»™å®šçš„è¯å…ƒåºåˆ—ï¼Œå…¶ä»»æ„ä¸€ä¸ªè¯å…ƒä½œä¸ºqueryï¼Œå…¶ä½™æ¯ä¸ªè¯å…ƒéƒ½ä½œä¸ºkeyå’Œvalueã€‚

è¾“å‡ºä¸ºï¼š

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015192516376.png" alt="image-20211015192516376" style="zoom: 67%;" />

```python
num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                                   num_hiddens, num_heads, 0.5)

batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
attention(X,X,X,valid_lens) #query,key,valueå…¨æ˜¯Xæœ¬èº«
```

### ä½ç½®ç¼–ç 

åœ¨å¤„ç†è¯å…ƒåºåˆ—æ—¶ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œæ˜¯é€ä¸ªçš„é‡å¤åœ°å¤„ç†è¯å…ƒçš„ï¼Œè€Œè‡ªæ³¨æ„åŠ›åˆ™å› ä¸ºå¹¶è¡Œè®¡ç®—è€Œæ”¾å¼ƒäº†é¡ºåºæ“ä½œã€‚ä¸ºäº†ä½¿ç”¨åºåˆ—çš„é¡ºåºä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è¾“å…¥è¡¨ç¤ºä¸­æ·»åŠ  *ä½ç½®ç¼–ç *ï¼ˆpositional encodingï¼‰æ¥æ³¨å…¥ç»å¯¹çš„æˆ–ç›¸å¯¹çš„ä½ç½®ä¿¡æ¯ã€‚

<img src="æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211015210117456.png" alt="image-20211015210117456" style="zoom:67%;" />

```python
class PositionalEncoding(nn.Module):
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„ `P`
        self.P = torch.zeros((1, max_len, num_hiddens))
        # ä¸‹é¢è¿™é‡Œå…ˆç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆmax_lenï¼Œ1ï¼‰çš„å¼ é‡ï¼Œç„¶åå¯¹å¼ é‡ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½
        #é™¤ä»¥ä¸‹é¢ç”Ÿæˆçš„num_hiddens/2ä¸ªæ•°ï¼Œæœ€åXçš„å½¢çŠ¶ä¸º(max_len,num_hiddens/2)
        X = torch.arange(max_len, dtype=torch.float32).reshape(
            -1, 1) / torch.pow(10000, torch.arange(
            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        self.P[:, :, 0::2] = torch.sin(X)
        self.P[:, :, 1::2] = torch.cos(X)

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)
```

## æ‰¹é‡å½’ä¸€åŒ–ä¸å±‚å½’ä¸€åŒ– batch normalization and layer normalization

### äºŒç»´

![image-20211219225542366](æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211219225542366.png)

![image-20211219225608181](æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211219225608181.png)

### ä¸‰ç»´

![image-20211219230608952](æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211219230608952.png)

![image-20211219230742759](æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211219230742759.png)

**åœ¨æ–‡æœ¬ä¸­layer normæ¯”batch normç”¨çš„å¤š**

![image-20211219231629570](æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211219231629570.png)

## transformer

![image-20211220134726686](æ³¨æ„åŠ›æœºåˆ¶.assets/image-20211220134726686.png)

### åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ

åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œå¯¹åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼ˆè¿™ä¸ªä½ç½®æŒ‡çš„æ˜¯è¾“å…¥Xçš„ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´æ‰€ä»£è¡¨çš„ä¸œè¥¿ï¼‰çš„è¡¨ç¤ºè¿›è¡Œå˜æ¢æ—¶ä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚è¾“å…¥ `X` çš„å½¢çŠ¶ï¼ˆæ‰¹é‡å¤§å°ã€æ—¶é—´æ­¥æ•°æˆ–åºåˆ—é•¿åº¦ã€éšå•å…ƒæ•°æˆ–ç‰¹å¾ç»´åº¦ï¼‰å°†è¢«ä¸€ä¸ªä¸¤å±‚çš„æ„ŸçŸ¥æœºè½¬æ¢æˆå½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ã€æ—¶é—´æ­¥æ•°ã€`ffn_num_outputs`ï¼‰çš„è¾“å‡ºå¼ é‡ã€‚**ä½œç”¨æ˜¯å¯¹æ‰€æœ‰çš„åºåˆ—ä½ç½®çš„è¡¨ç¤ºè¿›è¡Œè½¬æ¢ã€‚**

```python
class PositionWiseFFN(nn.Module):
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))
```

### æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–

æ‰¹é‡å½’ä¸€åŒ–æ˜¯åœ¨é€šé“ç»´è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå³å¯¹æ¯ä¸ªé€šé“è¿›è¡Œå½’ä¸€åŒ–ï¼‰ï¼Œå±‚å½’ä¸€åŒ–æ˜¯åœ¨ç‰¹å¾ç»´è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå³å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼‰

æ®‹å·®è¿æ¥è¦æ±‚ä¸¤ä¸ªè¾“å…¥çš„å½¢çŠ¶ç›¸åŒï¼Œä»¥ä¾¿[**åŠ æ³•æ“ä½œåè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ç›¸åŒ**]ã€‚

```python
class AddNorm(nn.Module):
    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)

add_norm = AddNorm([3, 4], 0.5) # Normalized_shape is input.size()[1:]
add_norm.eval()
add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape
#torch.Size([2, 3, 4])
```



### ç¼–ç å™¨

ç¼–ç å™¨ä¸­çš„transformerå—ï¼š

```python
class EncoderBlock(nn.Module):
    def __init__(self,key_size,query_size,value_size, 
                 num_hiddens,norm_shape,num_heads, dropout,
                 ffn_num_input,ffn_num_hiddens,use_bias=False, **kwargs):
        super().__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size,query_size,value_size, num_hiddens,num_heads, dropout)
        self.ffn = d2l.PositionWiseFFN(ffn_num_input, 
                                       ffn_num_hiddens, num_hidden)
        self.layerNorm1 = d2l.AddNorm(norm_shape,dorpout)
        self.layerNorm2 = d2l.AddNorm(norm_shape,dorpout)
    def forward(self,X,valid_len):
        Y = self.layerNorm1(X,self.attention(X,X,X,valid_len))
        return self.layerNorm2(Y,self.ffn(Y))
```

ç¼–ç å™¨éƒ¨åˆ†ï¼š

```python
class TransformerEncoder(nn.Module):
    def __init__(self,vocab_size,num_hiddens,key_size,query_size,
                 value_size,norm_shape,num_heads, dropout,ffn_num_input,
                 ffn_num_hiddens,num_layers,use_bias=False,**kwargs):
        super().__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Eembedding(vocab_size,num_hiddens)
        self.positionEncoder = d2l.PositionalEncoding(num_hiddens,dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                                EncoderBlock(key_size,query_size,value_size, 
                 num_hiddens,norm_shape,num_heads, dropout,
                 ffn_num_input,ffn_num_hiddens,use_bias))
    def forward(self,X,valid_len):
        X = self.positionEncoder(self.embedding(X)*math.sqrt(self.num_hiddens))
        for i,blk in enumerate(self.blks):
            X = blk(X,valid_len)
        return X
```



### è§£ç å™¨

è§£ç å™¨ä¸­çš„transformerå—ï¼š

```python
class DecoderBlock(nn.Module):
    def __init__(self,key_size,query_size,value_size,num_hiddens,
                 num_heads,norm_shape,dropout,ffn_num_input,
                 ffn_num_hiddens,i,**kwargs):
        super().__init__(**kwargs)
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(
        key_size,query_size,value_size,num_hiddens,num_heads,dropout)
        self.addNorm1 = d2l.AddNorm(norm_shape,dropout)
        self.attention = d2l.MultiHeadAttention(
        key_size,query_size,value_size,num_hiddens,num_heads,dropout)
        self.addNorm2 = d2l.AddNorm(norm_shape,dropout)
        self.ffn = d2l.PositionWiseFFN(ffn_num_input,
                                       ffn_num_hiddens,num_hiddens)
        self.addNorm3 = d2l.AddNorm(norm_shape,dropout)
    def forward(self,X,state):
        #stateç¬¬ä¸€ä¸ªæ˜¯encoderçš„éšå±‚è¾“å‡ºï¼Œç¬¬äºŒä¸ªæ˜¯encoderçš„valid_lenï¼Œ
        #ç¬¬ä¸‰ä¸ªæ˜¯decoderçš„å½“å‰å—çš„å½“å‰æ—¶é—´æ­¥çš„å‰é¢æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡º
        enc_output,enc_valid_lens = state[0],state[1]
        if state[2][self.i] is None:
            #å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œstateå°±åˆå§‹åŒ–ä¸ºdecoderçš„è¾“å…¥
            key_values = X
        else:
            key_values = torch.concat((state[2][self.i],X),axis=1)
        state[2][self.i] = key_values
        # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œè§£ç å™¨çš„valid_lenæ¯è¡Œå°±åº”è¯¥æ˜¯é€’å¢çš„æ•°ç»„ï¼Œ
        # å…¶å½¢çŠ¶ä¸º(batch_size,num_Steps)
        if self.train:
            batch_size,num_steps = X.shape
            dec_valid_len = torch.range(1,num_step+1,device=X.device)
            .repeat(batch_size,1)
        else:
            dec_valid_len = None
        X2 = self.attention1(X,key_values,key_values,dec_valid_len)
        Y = self.addNorm(X,X2)
        Y2 = self.attention2(Y,enc_output,enc_output,enc_valid_len)
        Z = self.addNorm(Y,Y2)
        return self.addNorm(Z,self.ffn(Z))
```

å®Œæ•´çš„transformerè§£ç å™¨ï¼š

```python
class TransformerDecoder(nn.Module):
    def __init__(self,dec_vocab_size,key_size,query_size,value_size,num_hiddens,
                    num_heads,norm_shape,dropout,ffn_num_input,
                 ffn_num_hiddens,num_layers,**kwargs):
        super().__init(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(dec_vocab_size,num_hiddens)
        self.positionEncoder = d2l.PositionalEncoding(num_hiddens,dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("blk"+str(i),
                    DecoderBlock(key_size,query_size,value_size,num_hiddens,
                    num_heads,norm_shape,dropout,ffn_num_input,
                 ffn_num_hiddens,i))
        self.dense = nn.Linear(num_hiddens,dec_vocab_size)
        
    def init_state(self,enc_outputs,enc_valid_lens,*args):
        return [enc_outputs,enc_valid_lens,[None]*self.num_layers]
    def forward(self,X,state):
        X = self.positionEncoder(self.embedding(X)*math.sqrt(self.num_hiddens))
        for i,blk in enumerate(self.blks):
            X,state = blk(X,state)
        return self.dense(X),state
```

### è®­ç»ƒ

dec_vocab_size,key_size,query_size,value_size,num_hiddens,
                    num_heads,norm_shape,dropout,ffn_num_input,
                 ffn_num_hiddens,num_layers

```python
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
key_size, query_size, value_size = 32, 32, 32
norm_shape = [32]

train_iter,src_vocab,tgt_vocab = d2l.load_data_nmt(batch_size,num_steps)

encoder = TransformerEncoder(len(src_vocab),num_hiddens,key_size, 
                             query_size, value_size,norm_shape,num_heads,
                            dropout,ffn_num_input,ffn_num_hiddens,num_layers)
decoder = TransformerDecoder(len(tgt_vocab),key_size, query_size, value_size,
                            num_hiddens,num_heads,norm_shape,dropout,
                            ffn_num_input,ffn_num_hiddens,num_layers)
net = d2l.EncoderDecoder(encoder,decoder)
d2l.train_seq2seq(net,train_iter,lr,num_epochs,tgt_vocab,device)
```

