## æ³¨æ„åŠ›æœºåˆ¶ç»Ÿè®¡å­¦åŸºç¡€

é—®é¢˜å¼•å…¥ï¼šcnnä¸­çš„å·ç§¯å±‚æ˜¯ç”¨æ¥æå–ç‰¹å¾çš„ï¼Œä½†è¿™å±äºè¢«åŠ¨æå–ç‰¹å¾ï¼Œå°±æ˜¯ä»€ä¹ˆç‰¹å¾æ›´æ˜æ˜¾å°±æå–ä»€ä¹ˆï¼Œç½‘ç»œæœ¬èº«æ˜¯ä¸çŸ¥é“è‡ªå·±iæƒ³è¦ä»€ä¹ˆçš„ã€‚æ‰€ä»¥ç»Ÿè®¡å­¦ä¸Šè®¤ä¸ºåº”è¯¥å°†è‡ªèº«éœ€è¦è€ƒè™‘è¿›å»ï¼Œä¹Ÿå°±æ˜¯é€‰æ‹©æœ€åˆé€‚çš„ï¼Œè€Œä¸æ˜¯é€‰æ‹©æœ€æ˜æ˜¾çš„ã€‚

### Nadaraya-Watson æ ¸å›å½’

**éå‚æ•°æ³¨æ„åŠ›æ±‡èš**

è¯¥æ–¹æ³•ä¼šå¯¹ä¸è‡ªå·±æœ€ç›¸è¿‘çš„æ ·æœ¬ç»™è¾ƒé«˜çš„æƒå€¼ï¼Œè€Œè¾ƒè¿œçš„æ ·æœ¬ç»™å°½å¯èƒ½ä½çš„æƒå€¼

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015142054872.png" alt="image-20211015142054872" style="zoom: 67%;" />

**å¸¦å‚æ•°æ³¨æ„åŠ›æ±‡èš**

æ ¸å‡½æ•°é€‰æ‹©é«˜æ–¯æ ¸å‡½æ•°

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015142215096.png" alt="image-20211015142215096" style="zoom:67%;" />



### æ‰¹é‡çŸ©é˜µä¹˜æ³•

**å‡å®šä¸¤ä¸ªå¼ é‡çš„å½¢çŠ¶åˆ†åˆ«æ˜¯ (ğ‘›,ğ‘,ğ‘) å’Œ (ğ‘›,ğ‘,ğ‘) ï¼Œå®ƒä»¬çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•è¾“å‡ºçš„å½¢çŠ¶ä¸º (ğ‘›,ğ‘,ğ‘)**

```python
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape
#torch.Size([2, 1, 6])
```

åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„èƒŒæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥[**ä½¿ç”¨å°æ‰¹é‡çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—å°æ‰¹é‡æ•°æ®ä¸­çš„åŠ æƒå¹³å‡å€¼**]ã€‚

```python
weights = torch.ones((2, 10)) * 0.1
values = torch.arange(20.0).reshape((2, 10))
#ä¸‹é¢è¿™ä¸€æ­¥å‡ç»´çš„æ“ä½œï¼šweightsä»(2ï¼Œ10)->(2,1,10)  valuesä»(2,10) -> (2,10,1)
#æ‰¹é‡çŸ©é˜µè®¡ç®—åå˜æˆ(2,1,1)
torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))
#tensor([[[ 4.5000]],
#        [[14.5000]]])
```



## æ³¨æ„åŠ›åˆ†æ•°

æ³¨æ„åŠ›åˆ†æ•°ä¹Ÿå°±æ˜¯ä¸‹é¢æ³¨æ„åŠ›æ±‡èšå±‚ä¸­çš„aï¼Œç°åœ¨è€ƒè™‘æˆå‘é‡

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015142952185.png" alt="image-20211015142952185" style="zoom:80%;" />

å…¶ä¸­ï¼š

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015143115909.png" alt="image-20211015143115909" style="zoom:67%;" />

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015143200139.png" alt="image-20211015143200139" style="zoom:67%;" />

### é®è”½softmaxæ“ä½œ

å‰é¢çš„softmaxæ“ä½œæ˜¯queryå¯¹æ‰€æœ‰çš„keyéƒ½æ“ä½œä¸€éï¼Œä½†å¾ˆå¤šæ—¶å€™æœ‰äº›keyæ˜¯ä¸éœ€è¦è®¡ç®—æƒé‡çš„ï¼Œå°±æ¯”å¦‚è¯´å‰é¢çš„ä¸ºäº†ä¿è¯æ¯ä¸ªæ ·æœ¬çš„æ—¶é—´æ­¥ä¸€æ ·ï¼Œå¯¹å¥å­åšäº†å¡«å……å’Œè£å‰ªã€‚é‚£ä¹ˆè¿™äº›å¡«å……éƒ¨åˆ†çš„æ—¶é—´æ­¥å°±æ˜¯é‚£äº›ä¸éœ€è¦ç®—æƒé‡çš„keyã€‚å› æ­¤è¦æŠŠä»–ä»¬ç»™maskäº†

```python
def masked_softmax(X,valid_len):
    # è¿™é‡ŒXçš„å½¢çŠ¶æ˜¯ï¼ˆbatch_size,query_num,key_numï¼‰ valid_lenæ˜¯ä¸€ç»´æˆ–äºŒç»´
    if valid_len is None:
        return nn.functional.softmax(X,dim=-1)
    else:
        shape = X.shape
        if valid_len.dim() == 1:
            valid_len = torch.repeat_interleave(valid_len,shape[1])
        else:
            valid_len = valid_len.reshape(-1)
        #sequence_maskä¸­çš„è¾“å…¥æ˜¯äºŒç»´çš„ï¼Œæ ·æœ¬æ•°Xæ—¶é—´æ­¥é•¿åº¦
        X = d2l.sequence_mask(X.reshape(-1,shape[-1]),valid_len,value=-1e6)
        return nn.functional.softmax(X.reshape(shape,dim=-1)
```



### åŠ æ€§æ³¨æ„åŠ›

åŠ æ€§æ³¨æ„åŠ›å°±ç›¸å½“äºå°†Qå’ŒKæ‹¼æ¥èµ·æ¥æ”¾åˆ°ä¸€ä¸ªéšè—å±‚å¤§å°ä¸ºhã€è¾“å‡ºå±‚ä¸º1çš„MLPä¸­ï¼Œéšè—å±‚æ¿€æ´»å‡½æ•°ä½¿ç”¨tanhï¼ŒQåˆ°éšå±‚çš„æƒé‡å°±æ˜¯Wqï¼ˆå¤§å°ä¸ºh,qï¼‰ï¼ŒKåˆ°éšå±‚çš„æƒé‡å°±æ˜¯Wkï¼ˆå¤§å°ä¸ºh,kï¼‰ï¼Œéšå±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ä¸ºWv (å¤§å°ä¸ºh)

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015143347547.png" alt="image-20211015143347547" style="zoom:67%;" />

```python
class AdditiveAttention(nn.Module):
    def __init__(self,key_Size,query_size,num_hiddens,dorpout,**kwargs):
        super().__init__(**kwargs)
        self.W_q = nn.Linear(query_size,num_hiddens,bias=False)
        self.W_k = nn.Linear(key_Size,num_hiddens,bias=False)
        self.W_v = nn.Linear(num_hiddens,1,bias=False)
    def forward(self,query,key,values,valid_lens):
        # querysçš„å½¢çŠ¶ï¼š(batch_size,query_num,num_hiddens)
        # keysçš„å½¢çŠ¶ï¼š(batch_size,key_num,num_hiddens)
        querys,keys = self.W_q(query),self.W_k(key)
        #featuresçš„å½¢çŠ¶ï¼š(batch_size,query_numï¼Œkey_num,num_hiddens)
        features = querys.unsqueeze(2)+keys.unsqueeze(1)
        features = torch.tanh(features)
        #scoresï¼š(batch_size,query_numï¼Œkey_num),æ¯ä¸ªæ•°éƒ½æ˜¯ä¸€ä¸ªscore
        scores = self.W_v(features).squeeze(-1)
        self.attention_weight = d2l.masked_softmax(score,valid_lens)
        #valueå½¢çŠ¶ä¸º(batch_size,value_num,embedding_size)
        #è¯¥å‡½æ•°è¾“å‡ºå½¢çŠ¶ä¸ºï¼š(batch_size,query_num,embedding_size)
        return torch.bmm(self.attention_weight,values)
```

### ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

ä½¿ç”¨ç‚¹ç§¯å¯ä»¥å¾—åˆ°è®¡ç®—æ•ˆç‡æ›´é«˜çš„è¯„åˆ†å‡½æ•°ã€‚ä½†æ˜¯ç‚¹ç§¯æ“ä½œè¦æ±‚æŸ¥è¯¢å’Œé”®å…·æœ‰ç›¸åŒçš„é•¿åº¦ ğ‘‘

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015161248648.png" alt="image-20211015161248648" style="zoom:67%;" />

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015161304048.png" alt="image-20211015161304048" style="zoom:67%;" />

```python
class DotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # è®¾ç½® `transpose_b=True` ä¸ºäº†äº¤æ¢ `keys` çš„æœ€åä¸¤ä¸ªç»´åº¦
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
```



## åœ¨seq2seqæ¨¡å‹ä¸Šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015161450119.png" alt="image-20211015161450119" style="zoom:67%;" />

### Bahdanau æ³¨æ„åŠ›

å°†æ³¨æ„åŠ›æœºåˆ¶ç”¨åˆ°seq2seqæ¨¡å‹å«åšBahdanau æ³¨æ„åŠ›ã€‚è¿™ä¸ªæ¨¡å‹åªéœ€è¦ä¿®æ”¹seq2seqçš„è§£ç å™¨ã€‚

**å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨åŸºæœ¬æ¥å£**

```python
class AttentionDecoder(d2l.Decoder):
    """å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨åŸºæœ¬æ¥å£"""
    def __init__(self, **kwargs):
        super(AttentionDecoder, self).__init__(**kwargs)
    @property
    def attention_weights(self):
        raise NotImplementedError
```

**å®ç°å¸¦æœ‰Bahdanauæ³¨æ„åŠ›çš„å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨**

```python
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self,vocab_size,embedding_size,num_hiddens,
                 num_layers,dropout,**kwargs):
        super().__init__(**kwargs)
        self.attention=d2l.AdditiveAttention(num_hiddens,num_hiddens,
                                             num_hiddens,dropout)
        self.embedding=nn.Embedding(vocab_size,embedding_size)
        self.rnn=nn.GRU(embedding_size+num_hiddens,num_hiddens,num_layers,
                        dropout=dropout)
        self.dense=nn.Linear(num_hiddens,vocab_size)
        
    def init_state(self,enc_outputs,enc_valid_len,*args):
        outputs,hidden_state = enc_outputs
        return (outputs.permute(1,0,2),hidden_state,enc_valid_len)
    def forward(self,X,state):
        #enc_outputså½¢çŠ¶ä¸ºï¼šbatch_size,num_steps,num_hiddens
        #hidden_stateå½¢çŠ¶ä¸ºï¼šbatch_size,num_layers,num_hiddens
        #enc_valid_lenå½¢çŠ¶ä¸ºï¼šbatch_size
        enc_outputs,hidden_state,enc_valid_len = state
        X = self.embedding(X).permite(1,0,2)
        outputs,self._attention_weights = [],[]
        # Xçš„å½¢çŠ¶ (`num_steps`, `batch_size`, `embed_size`)
        for x in X:
            query = torch.unsqueeze(hidden_state[-1],dim=-1)
            context = self.attention(query,enc_outputs,enc_outputs,enc_valid_len)
            x = torch.cat((context,torch.unsqueeze(x,dim=1)),dim=-1)
            out,hidden_state = self.rnn(x.permute(1,0,2),hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        outputs = self.dense(torch.cat(outputs),dim=0)
        return outputs.permute(1,0,2),[enc_outputs,hidden_state,enc_valid_len]
            
    @property 
    def attention_weight(self):
        return self._attention_weights
```

**è®­ç»ƒ**

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 250, d2l.try_gpu()

train_iter,src_vocab,tgt_vocab = d2l.load_data_nmt(batch_size,num_steps)
encoder = d2l.Seq2SeqEncoder(
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net,train_iter,num_epoch,tgt_vocab,device)
```



## å¤šå¤´æ³¨æ„åŠ›

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015190433700.png" alt="image-20211015190433700" style="zoom:67%;" />

```python
#@save
class MultiHeadAttention(nn.Module):
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        output = self.attention(queries, keys, values, valid_lens)

        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
```



## è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç 

### è‡ªæ³¨æ„åŠ›

è‡ªæ³¨æ„åŠ›å°±æ˜¯å¯¹äºä¸€ä¸ªç»™å®šçš„è¯å…ƒåºåˆ—ï¼Œå…¶ä»»æ„ä¸€ä¸ªè¯å…ƒä½œä¸ºqueryï¼Œå…¶ä½™æ¯ä¸ªè¯å…ƒéƒ½ä½œä¸ºkeyå’Œvalueã€‚

è¾“å‡ºä¸ºï¼š

<img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20211015192516376.png" alt="image-20211015192516376" style="zoom: 67%;" />

```python
num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                                   num_hiddens, num_heads, 0.5)

batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
attention(X,X,X,valid_lens) #query,key,valueå…¨æ˜¯Xæœ¬èº«
```

### ä½ç½®ç¼–ç 

åœ¨å¤„ç†è¯å…ƒåºåˆ—æ—¶ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œæ˜¯é€ä¸ªçš„é‡å¤åœ°å¤„ç†è¯å…ƒçš„ï¼Œè€Œè‡ªæ³¨æ„åŠ›åˆ™å› ä¸ºå¹¶è¡Œè®¡ç®—è€Œæ”¾å¼ƒäº†é¡ºåºæ“ä½œã€‚ä¸ºäº†ä½¿ç”¨åºåˆ—çš„é¡ºåºä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è¾“å…¥è¡¨ç¤ºä¸­æ·»åŠ  *ä½ç½®ç¼–ç *ï¼ˆpositional encodingï¼‰æ¥æ³¨å…¥ç»å¯¹çš„æˆ–ç›¸å¯¹çš„ä½ç½®ä¿¡æ¯ã€‚



## transformer

### åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ

### æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–